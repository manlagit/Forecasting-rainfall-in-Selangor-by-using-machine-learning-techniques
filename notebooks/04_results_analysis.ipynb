{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Results Analysis and Reporting\n",
    "\n",
    "This notebook covers the analysis of model results from Step 3, generation of visualizations, and preparation for integrating findings into the final LaTeX report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tikzplotlib # For LaTeX compatible plots\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib # For loading saved models if needed for feature importance\n",
    "\n",
    "# Define base directory and paths\n",
    "BASE_DIR = os.path.join(os.path.dirname(os.getcwd()), '') # Assumes notebook is in 'notebooks' subdir\n",
    "RESULTS_PATH = os.path.join(BASE_DIR, 'results')\n",
    "FIGURES_PATH = os.path.join(BASE_DIR, 'reports', 'figures')\n",
    "MODELS_PATH = os.path.join(BASE_DIR, 'models', 'saved_models')\n",
    "\n",
    "# Ensure figures directory exists\n",
    "os.makedirs(FIGURES_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Results Path: {RESULTS_PATH}\")\n",
    "print(f\"Figures Path: {FIGURES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Review Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model comparison metrics (e.g., RMSE, MAE, R2)\n",
    "try:\n",
    "    comparison_df = pd.read_csv(os.path.join(RESULTS_PATH, \"model_comparison.csv\"))\n",
    "    print(\"--- Model Performance Comparison (Metrics) ---\")\n",
    "    print(comparison_df)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: model_comparison.csv not found in {RESULTS_PATH}. Please run Step 3 first.\")\n",
    "    comparison_df = pd.DataFrame() # Create empty df to avoid later errors\n",
    "\n",
    "# Load model predictions\n",
    "try:\n",
    "    predictions_df = pd.read_csv(os.path.join(RESULTS_PATH, \"model_predictions.csv\"))\n",
    "    # Ensure Date column is parsed as datetime if it's not already\n",
    "    if 'Date' in predictions_df.columns:\n",
    "        predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])\n",
    "    print(\"\\n--- Model Predictions (First 5 rows) ---\")\n",
    "    print(predictions_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: model_predictions.csv not found in {RESULTS_PATH}. Please run Step 3 first.\")\n",
    "    predictions_df = pd.DataFrame() # Create empty df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Metrics\n",
    "*(Markdown cell for observations)*\n",
    "\n",
    "- Examine the `comparison_df` to identify top-performing models based on RMSE, MAE, R2.\n",
    "- Note any significant differences in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Comparison Plot (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comparison_df.empty and 'RMSE' in comparison_df.columns and 'Model' in comparison_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Sort by RMSE for better visualization\n",
    "    sorted_comparison_df = comparison_df.sort_values(by='RMSE', ascending=True)\n",
    "    sns.barplot(x='Model', y='RMSE', data=sorted_comparison_df)\n",
    "    plt.title('Model Comparison: Root Mean Squared Error (RMSE)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('RMSE (mm)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    \n",
    "    # Save as PNG\n",
    "    rmse_comparison_png_path = os.path.join(FIGURES_PATH, \"model_comparison_rmse.png\")\n",
    "    plt.savefig(rmse_comparison_png_path, dpi=300)\n",
    "    print(f\"RMSE comparison plot saved to {rmse_comparison_png_path}\")\n",
    "    \n",
    "    # Save as TikZ for LaTeX\n",
    "    rmse_comparison_tex_path = os.path.join(FIGURES_PATH, \"model_comparison_rmse.tex\")\n",
    "    try:\n",
    "        tikzplotlib.save(rmse_comparison_tex_path)\n",
    "        print(f\"RMSE comparison plot saved for LaTeX: {rmse_comparison_tex_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save TikZ plot for RMSE comparison: {e}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping RMSE comparison plot: comparison_df is empty or missing 'RMSE'/'Model' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predictions vs Actual Plot (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comparison_df.empty and not predictions_df.empty and 'RMSE' in comparison_df.columns:\n",
    "    # Determine the best model based on the lowest RMSE\n",
    "    best_model_name = comparison_df.sort_values(by='RMSE').iloc[0]['Model']\n",
    "    print(f\"Best model based on RMSE: {best_model_name}\")\n",
    "\n",
    "    if 'Actual' in predictions_df.columns and f'{best_model_name}_pred' in predictions_df.columns and 'Date' in predictions_df.columns:\n",
    "        actual_values = predictions_df['Actual']\n",
    "        predicted_values = predictions_df[f'{best_model_name}_pred']\n",
    "        dates = predictions_df['Date']\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(dates, actual_values, label='Actual Rainfall', color='blue', marker='.', linestyle='-')\n",
    "        plt.plot(dates, predicted_values, label=f'Predicted Rainfall ({best_model_name})', color='orange', linestyle='--')\n",
    "        plt.title(f'Actual vs. Predicted Rainfall ({best_model_name})')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Precipitation (mm)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save as PNG\n",
    "        pred_actual_png_path = os.path.join(FIGURES_PATH, \"predictions_vs_actual.png\")\n",
    "        plt.savefig(pred_actual_png_path, dpi=300)\n",
    "        print(f\"Predictions vs Actual plot saved to {pred_actual_png_path}\")\n",
    "        \n",
    "        # Save as TikZ for LaTeX\n",
    "        pred_actual_tex_path = os.path.join(FIGURES_PATH, \"predictions_vs_actual.tex\")\n",
    "        try:\n",
    "            tikzplotlib.save(pred_actual_tex_path)\n",
    "            print(f\"Predictions vs Actual plot saved for LaTeX: {pred_actual_tex_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save TikZ plot for Predictions vs Actual: {e}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Skipping Predictions vs Actual plot: Columns for best model '{best_model_name}' or 'Actual' or 'Date' not found in predictions_df.\")\n",
    "else:\n",
    "    print(\"Skipping Predictions vs Actual plot: comparison_df or predictions_df is empty or RMSE column missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Importance Plot (Example for a loaded model)\n",
    "\n",
    "This requires loading a saved model (e.g., RandomForest or XGBoost) and its corresponding feature names. \n",
    "The `train_models.py` script would need to ensure feature names are available or saved alongside models.\n",
    "For now, this is a placeholder. You might need to adapt `src/visualization/visualize.py` or add logic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Feature Importance Plot\n",
    "# Example: Assuming 'RandomForest' was the best and its features are known\n",
    "# best_model_name_for_fi = 'RandomForest' # Or XGBoost, etc.\n",
    "# try:\n",
    "#     model_path = os.path.join(MODELS_PATH, f\"{best_model_name_for_fi.lower()}_model.pkl\")\n",
    "#     loaded_model = joblib.load(model_path)\n",
    "#     print(f\"Loaded model {best_model_name_for_fi} from {model_path}\")\n",
    "    \n",
    "#     # You need the feature names used for training this model\n",
    "#     # This might come from the 'engineered_features.csv' columns (excluding target/date)\n",
    "#     # Or saved separately during training\n",
    "#     engineered_df = pd.read_csv(os.path.join(BASE_DIR, 'data', 'processed', 'engineered_features.csv'))\n",
    "#     feature_names = [col for col in engineered_df.columns if col not in ['Date', 'Precipitation_mm', 'Year', 'Week_Number']]\n",
    "    \n",
    "#     if hasattr(loaded_model, 'feature_importances_'):\n",
    "#         importances = loaded_model.feature_importances_\n",
    "#         sorted_indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         plt.title(f'Feature Importance for {best_model_name_for_fi}')\n",
    "#         plt.bar(range(len(feature_names)), importances[sorted_indices], align='center')\n",
    "#         plt.xticks(range(len(feature_names)), np.array(feature_names)[sorted_indices], rotation=90)\n",
    "#         plt.tight_layout()\n",
    "#         fi_png_path = os.path.join(FIGURES_PATH, \"feature_importance.png\")\n",
    "#         plt.savefig(fi_png_path, dpi=300)\n",
    "#         print(f\"Feature importance plot saved to {fi_png_path}\")\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(f\"Model {best_model_name_for_fi} does not have 'feature_importances_' attribute.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Model file for {best_model_name_for_fi} not found. Skipping feature importance plot.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error generating feature importance plot: {e}\")\n",
    "print(\"Feature importance plot generation is currently a placeholder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results and Document Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model Comparison Insights\n",
    "*(Markdown cell for observations)*\n",
    "\n",
    "- Which model performed best overall based on regression metrics (RMSE, MAE, R2)?\n",
    "- Were there any surprising results?\n",
    "- How do the models compare in terms of complexity vs. performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Classification Metrics (AUC-ROC Example)\n",
    "\n",
    "If the project includes a classification task (e.g., predicting rain/no-rain), calculate AUC-ROC. This assumes 'Precipitation_mm' > 0.1mm signifies a rain event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comparison_df.empty and not predictions_df.empty and 'RMSE' in comparison_df.columns:\n",
    "    best_model_name = comparison_df.sort_values(by='RMSE').iloc[0]['Model']\n",
    "    \n",
    "    if 'Actual' in predictions_df.columns and f'{best_model_name}_pred' in predictions_df.columns:\n",
    "        y_test_binary = (predictions_df['Actual'] > 0.1).astype(int)\n",
    "        \n",
    "        # For regression models, their continuous predictions can be used as scores.\n",
    "        # Higher predicted rainfall amount implies higher likelihood of 'rain' class.\n",
    "        # Ensure predictions are non-negative if they can be negative (like ARIMA).\n",
    "        y_pred_scores = predictions_df[f'{best_model_name}_pred']\n",
    "        if (y_pred_scores < 0).any():\n",
    "             y_pred_scores = np.maximum(0, y_pred_scores) # Clip negative predictions at 0\n",
    "        \n",
    "        if len(np.unique(y_test_binary)) > 1: # Check for at least two classes in true labels\n",
    "            try:\n",
    "                auc_roc = roc_auc_score(y_test_binary, y_pred_scores)\n",
    "                print(f\"\\nAUC-ROC for {best_model_name} (treating regression output as classification score): {auc_roc:.4f}\")\n",
    "                \n",
    "                # Add to comparison_df if not already there from train_models.py\n",
    "                if 'AUC_ROC' not in comparison_df.columns:\n",
    "                    comparison_df['AUC_ROC'] = np.nan \n",
    "                comparison_df.loc[comparison_df['Model'] == best_model_name, 'AUC_ROC'] = auc_roc\n",
    "                print(\"Updated comparison_df with AUC-ROC for best model:\")\n",
    "                print(comparison_df)\n",
    "            except ValueError as e:\n",
    "                print(f\"Could not calculate AUC-ROC for {best_model_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping AUC-ROC for {best_model_name}: y_test_binary has only one class.\")\n",
    "    else:\n",
    "        print(f\"Could not calculate AUC-ROC: Prediction column for {best_model_name} or 'Actual' column not found.\")\n",
    "else:\n",
    "    print(\"Skipping AUC-ROC calculation: comparison_df or predictions_df is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Overall Summary and Recommendations\n",
    "*(Markdown cell for final thoughts)*\n",
    "\n",
    "- Summarize the key findings from the model training and evaluation.\n",
    "- Discuss any limitations of the current models or analysis.\n",
    "- Propose next steps or future improvements (e.g., trying more complex models, further feature engineering, hyperparameter optimization for other models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare for LaTeX Report Update\n",
    "\n",
    "The generated figures (`.png` and `.tex` files) in `reports/figures/` can now be integrated into `reports/latex/rainfall_forecasting_report.tex`.\n",
    "\n",
    "Example LaTeX code for including a figure:\n",
    "```latex\n",
    "\\begin{figure}[h!]\n",
    "    \\centering\n",
    "    % For PNG images\n",
    "    % \\includegraphics[width=0.8\\textwidth]{../figures/model_comparison_rmse.png}\n",
    "    % For TikZ/PGFPlots (preferred for quality)\n",
    "    \\input{../figures/model_comparison_rmse.tex} \n",
    "    \\caption{Comparison of Model Performance Based on RMSE}\n",
    "    \\label{fig:model_comparison_rmse}\n",
    "\\end{figure}\n",
    "```\n",
    "Remember to update the main LaTeX file (`rainfall_forecasting_report.tex` or `expanded_report.tex`) with these figures and the textual analysis derived from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finalize and Save Notebook\n",
    "\n",
    "Ensure all cells have been run and outputs are visible. Save the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

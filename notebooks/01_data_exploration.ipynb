{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Rainfall Forecasting in Selangor\n",
    "\n",
    "This notebook explores the rainfall dataset to understand its structure, patterns, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df1 = pd.read_csv('../data/raw/230731665812CCD_weekly1.csv')\n",
    "df2 = pd.read_csv('../data/raw/230731450378CCD_weekly2.csv')\n",
    "\n",
    "print(\"Dataset 1 shape:\", df1.shape)\n",
    "print(\"Dataset 2 shape:\", df2.shape)\n",
    "print(\"\\nDataset 1 columns:\", list(df1.columns))\n",
    "print(\"Dataset 2 columns:\", list(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of each dataset\n",
    "print(\"First 5 rows of Dataset 1:\")\n",
    "display(df1.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of Dataset 2:\")\n",
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the datasets\n",
    "print(\"Dataset 1 Info:\")\n",
    "print(df1.info())\n",
    "\n",
    "print(\"\\nDataset 2 Info:\")\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in Dataset 1:\")\n",
    "print(df1.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Dataset 2:\")\n",
    "print(df2.isnull().sum())\n",
    "\n",
    "# Calculate missing value percentages\n",
    "print(\"\\nMissing value percentages in Dataset 1:\")\n",
    "print((df1.isnull().sum() / len(df1) * 100).round(2))\n",
    "\n",
    "print(\"\\nMissing value percentages in Dataset 2:\")\n",
    "print((df2.isnull().sum() / len(df2) * 100).round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for analysis\n",
    "# Assuming both datasets have similar structure\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "df_combined = df_combined.drop_duplicates()\n",
    "print(f\"After removing duplicates: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "display(df_combined.describe())\n",
    "\n",
    "# Focus on key weather variables\n",
    "weather_cols = ['Temp_avg', 'Relative_Humidity', 'Wind_kmh', 'Precipitation_mm']\n",
    "if all(col in df_combined.columns for col in weather_cols):\n",
    "    print(\"\\nWeather Variables Statistics:\")\n",
    "    display(df_combined[weather_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data ranges for outliers\n",
    "print(\"Data Ranges Analysis:\")\n",
    "for col in weather_cols:\n",
    "    if col in df_combined.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Min: {df_combined[col].min():.2f}\")\n",
    "        print(f\"  Max: {df_combined[col].max():.2f}\")\n",
    "        print(f\"  Range: {df_combined[col].max() - df_combined[col].min():.2f}\")\n",
    "        \n",
    "        # Check for potential outliers using IQR method\n",
    "        Q1 = df_combined[col].quantile(0.25)\n",
    "        Q3 = df_combined[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_combined[(df_combined[col] < lower_bound) | (df_combined[col] > upper_bound)]\n",
    "        print(f\"  Potential outliers: {len(outliers)} ({len(outliers)/len(df_combined)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime if exists\n",
    "if 'Date' in df_combined.columns:\n",
    "    df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "    df_combined = df_combined.sort_values('Date')\n",
    "    \n",
    "    print(f\"Date range: {df_combined['Date'].min()} to {df_combined['Date'].max()}\")\n",
    "    print(f\"Total time span: {(df_combined['Date'].max() - df_combined['Date'].min()).days} days\")\n",
    "    \n",
    "    # Check for date gaps\n",
    "    date_gaps = df_combined['Date'].diff().dt.days\n",
    "    print(f\"\\nDate gaps analysis:\")\n",
    "    print(f\"  Median gap: {date_gaps.median():.0f} days\")\n",
    "    print(f\"  Max gap: {date_gaps.max():.0f} days\")\n",
    "    print(f\"  Gaps > 7 days: {(date_gaps > 7).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}